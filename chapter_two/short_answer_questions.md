1. Why is there overfitting on the test set when it performs much better than the cross validation?

2. Explain the trade offs between grid search and randomized search for hyperparameter tuning.  When is one better than the other?

3. Look up and describe regression to the mean.

4. Suppose you just launched a machine learning system that tries to predict the probability someone will click on an advertisment.  Assume the website gets a billion views on average per month.  How would you monitor the model?  What measures are important?  How would you make sure your model is doing what you expect?  

5. Look up and describe the wisdom of crowds.

6. In the chapter ensembling is used to get better accuracy on the housing price prediction problem.  Why do you think it does better?  Is this just a fluke?  Should you always use ensembling?  What are the tradeoffs of ensembling versus not?  

7. Describe scaling, how and why is it useful?  Should you always scale your features?  Is there any tradeoff in scaling versus not?  Compare and contrast min max scaling and standard scaling as described by scikit-learn (subtract the mean and then devide by the standard deviation).  

8. Consider the pipeline used by scikit learn - describe at each step what happens.  Please come up with some example code and explain how does data flow into the pipeline and how does it flow out? Then explain this generally.  Are there any disadvantages to setting up a machine learning pipeline in this way?

9. In the chapter the kth norm is defined.  Why can't you set 0 < k < 1?  

10. In the chapter's main example, some of the attributes are combined.  Why is putting things in the same units important?  Does this always matter?  Can you think of an other example where units are important?  Can you think of a counter example, one where units don't matter at all?

